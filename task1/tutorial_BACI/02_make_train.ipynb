{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from types import SimpleNamespace\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SimpleNamespace(seed = 2024,\n",
    "                         learning_rate = 1e-3,\n",
    "                         weight_decay = 5e-4,\n",
    "                         batch_size = 16,\n",
    "                         print_interval = 10,\n",
    "                         input_dim = 1,\n",
    "                         output_dim = 1,\n",
    "                         hidden_size = 8,\n",
    "                         n_layers = 2,\n",
    "                         n_heads = 2,\n",
    "                         out_head = 1,\n",
    "                         num_epochs = 500,\n",
    "                         dropout = 0.6,\n",
    "                         patience = 10,\n",
    "                         checkpoints_dir = './ckp',\n",
    "                         device = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "                         conv_type = 'GAT',\n",
    "                         data_path = './data',\n",
    "                         data_name = 'baci',\n",
    "                         save_model = True,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.device == 'cuda':\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(args.checkpoints_dir):\n",
    "    os.makedirs(args.checkpoints_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames= [f'term_{num}' for num in range(3134)]\n",
    "\n",
    "X_train = pd.read_csv(\"./data/X_train.csv\", names=colnames, header=None)\n",
    "y_train = pd.read_csv(\"./data/y_train.csv\", names=['GDP'], header=None)\n",
    "\n",
    "X_test = pd.read_csv(\"./data/X_test.csv\", names=colnames, header=None)\n",
    "y_test = pd.read_csv(\"./data/y_test.csv\", names=['GDP'], header=None)\n",
    "\n",
    "X_val = pd.read_csv(\"./data/X_val.csv\", names=colnames, header=None)\n",
    "y_val = pd.read_csv(\"./data/y_val.csv\", names=['GDP'], header=None)\n",
    "\n",
    "edge_index = torch.load('./data/edge_index.pt')\n",
    "edge_attr = torch.load('./data/edge_attr.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "\n",
    "y_train = torch.tensor(np.log(y_train.values), dtype=torch.float32)\n",
    "y_val = torch.tensor(np.log(y_val.values), dtype=torch.float32)\n",
    "y_test = torch.tensor(np.log(y_test.values), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_patients_train = X_train.shape[0]\n",
    "num_patients_test = X_test.shape[0]\n",
    "num_patients_val = X_val.shape[0]\n",
    "\n",
    "# training set\n",
    "graphs_train = []\n",
    "for i in range(num_patients_train):\n",
    "    node_features = X_train[i]\n",
    "    target = y_train[i]\n",
    "    graph_train = (node_features, edge_index, edge_attr, target)\n",
    "    graphs_train.append(graph_train)\n",
    "\n",
    "# test set\n",
    "graphs_test = []\n",
    "for i in range(num_patients_test):\n",
    "    node_features = X_test[i]\n",
    "    target = y_test[i]\n",
    "    graph_test = (node_features, edge_index, edge_attr, target)\n",
    "    graphs_test.append(graph_test)\n",
    "\n",
    "# valid set\n",
    "graphs_val = []\n",
    "for i in range(num_patients_val):\n",
    "    node_features = X_val[i]\n",
    "    target = y_val[i]\n",
    "    graph_val = (node_features, edge_index, edge_attr, target)\n",
    "    graphs_val.append(graph_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = [Data(x=graph[0].reshape(len(graphs_train[0][0]), 1), edge_index=graph[1], edge_attr=graph[2], y=graph[3]) for graph in graphs_train]\n",
    "\n",
    "data_test = [Data(x=graph[0].reshape(len(graphs_test[0][0]), 1), edge_index=graph[1], edge_attr=graph[2], y=graph[3]) for graph in graphs_test]\n",
    "\n",
    "data_val = [Data(x=graph[0].reshape(len(graphs_val[0][0]), 1), edge_index=graph[1], edge_attr=graph[2], y=graph[3]) for graph in graphs_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(data_train, batch_size=args.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(data_test, batch_size=args.batch_size, shuffle=False)\n",
    "val_loader = DataLoader(data_val, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "for step, data in enumerate(train_loader):\n",
    "    data = data.to(args.device)\n",
    "\n",
    "    print('Training Batches: ')\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_channels, output_dim, num_heads, out_head, dropout):\n",
    "        super(GAT, self).__init__()\n",
    "        self.gat1 = GATConv(num_node_features, hidden_channels, edge_dim=1, heads=num_heads)\n",
    "        self.gat2 = GATConv(hidden_channels * num_heads, hidden_channels, edge_dim=1, heads=num_heads)\n",
    "        self.gat3 = GATConv(hidden_channels * num_heads, output_dim, edge_dim=1, concat=False, heads=out_head)\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.relu(self.gat1(x, edge_index, edge_attr))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.relu(self.gat2(x, edge_index, edge_attr))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.gat3(x, edge_index, edge_attr)\n",
    "\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAT(num_node_features=args.input_dim, hidden_channels=args.hidden_size, output_dim=args.output_dim, num_heads=args.n_heads, out_head=args.out_head, dropout=args.dropout)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "model = model.to(args.device)\n",
    "criterion = criterion.to(args.device)\n",
    "\n",
    "num_epochs = args.num_epochs\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "es_counter = 0\n",
    "best_loss = np.inf\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, data in enumerate(train_loader):\n",
    "        data = data.to(args.device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(data.x, data.edge_index, data.edge_attr)\n",
    "        loss = criterion(out, data.y.view(-1, 1).to(args.device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_train_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(average_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for step, data in enumerate(val_loader):\n",
    "            data = data.to(args.device)\n",
    "\n",
    "            out = model(data.x, data.edge_index, data.edge_attr)\n",
    "            loss = criterion(out, data.y.view(-1, 1).to(args.device))\n",
    "            val_loss += loss.item()\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "        average_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        if epoch % args.print_interval == 0:\n",
    "            print(f'Epoch: {epoch:03d}, Train loss: {average_train_loss:.4f}, Validation Loss: {average_val_loss:.4f}')\n",
    "\n",
    "    torch.save(model.state_dict(), f'./{args.checkpoints_dir}/{epoch}.pth')\n",
    "\n",
    "    if val_losses[-1] < best_loss:\n",
    "        best_loss = val_losses[-1]\n",
    "        best_epoch = epoch\n",
    "        es_counter = 0\n",
    "    else:\n",
    "        es_counter += 1\n",
    "    \n",
    "    if es_counter == args.patience:\n",
    "        print('Early Stopping!!')\n",
    "        break\n",
    "\n",
    "    files = glob.glob(f'./{args.checkpoints_dir}/*.pth')\n",
    "    for file in files:\n",
    "        epoch_nb = int(file.split('\\\\')[1].split('.')[0])\n",
    "        if epoch_nb < best_epoch:\n",
    "            os.remove(file)\n",
    "\n",
    "    files = glob.glob(f'./{args.checkpoints_dir}/*.pth')\n",
    "    for file in files:\n",
    "        epoch_nb = int(file.split('\\\\')[1].split('.')[0])\n",
    "        if epoch_nb > best_epoch:\n",
    "            os.remove(file)\n",
    "\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Time used for training: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(train_value, test_value):\n",
    "    plt.subplot(121)\n",
    "    plt.plot(train_value, label='Train Loss')\n",
    "    plt.title('Train Loss')\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.plot(test_value, label='Valid Loss')\n",
    "    plt.title('Valid Loss')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Loading {best_epoch}th epoch')\n",
    "test_model = GAT(num_node_features=args.input_dim, hidden_channels=args.hidden_size, output_dim=args.output_dim, num_heads=args.n_heads, out_head=args.out_head, dropout=args.dropout)\n",
    "test_model.load_state_dict(torch.load(f'./{args.checkpoints_dir}/{best_epoch}.pth'))\n",
    "test_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, criterion, args):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_losses = 0\n",
    "        for test_data in test_loader:\n",
    "            test_data.to(args.device)\n",
    "            test_logit = model(test_data.x, test_data.edge_index, test_data.edge_attr)\n",
    "            loss = criterion(test_logit, test_data.y.view(-1, 1).to(args.device))\n",
    "            test_losses += loss.item()\n",
    "\n",
    "    avg_test_loss = test_losses / len(test_loader)\n",
    "    print(f\"Test loss: {avg_test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(test_model, criterion, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
